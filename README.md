## ðŸ”§ Installation

- clone this repo:

```
git clone https://github.com/michael861227/Drag-Editing.git
cd Drag-Editing
git submodule update --init --recursive
```

- set up a new conda environment

```
conda env create --file environment.yaml
conda activate DYG
```

## ðŸ“š Data Preparation

Refer to [3DGS](https://github.com/graphdeco-inria/gaussian-splatting?tab=readme-ov-file#processing-your-own-scenes) for reconstruction. We recommend setting the spherical harmonic degree to 0 for the reconstruction process.
Alternatively, you can use the [data](https://drive.google.com/drive/folders/19Jv3crbF7xMu1ouNoCH-mEH87ClykpuY?usp=sharing) we have prepared. Taking the face scene as an example, the file structure is as follows:

```
â””â”€â”€ data
    â””â”€â”€ face
        â”œâ”€â”€ export_1
        â”‚   â”œâ”€â”€ drag_points.json
        â”‚   â””â”€â”€ gaussian_mask.pt
        â”œâ”€â”€ image
        â”œâ”€â”€ sparse
        â””â”€â”€ point_cloud.ply
```

Since we use LightningDrag as the diffusion prior, you need to download the model following the instructions in [lightningdrag](https://github.com/magic-research/LightningDrag/blob/main/INSTALLATION.md#2-download-pretrained-models) and organize it according to the following structure.

```
â””â”€â”€ checkpoints
    â”œâ”€â”€ dreamshaper-8-inpainting
    â”œâ”€â”€ lcm-lora-sdv1-5
    â”‚   â””â”€â”€ pytorch_lora_weights.safetensors
    â”œâ”€â”€ sd-vae-ft-ema
    â”‚   â”œâ”€â”€ config.json
    â”‚   â”œâ”€â”€ diffusion_pytorch_model.bin
    â”‚   â””â”€â”€ diffusion_pytorch_model.safetensors
    â”œâ”€â”€ IP-Adapter/models
    â”‚   â”œâ”€â”€ image_encoder
    â”‚   â””â”€â”€ ip-adapter_sd15.bin
    â””â”€â”€ lightning-drag-sd15
        â”œâ”€â”€ appearance_encoder
        â”‚   â”œâ”€â”€ config.json
        â”‚   â””â”€â”€ diffusion_pytorch_model.safetensors
        â”œâ”€â”€ point_embedding
        â”‚   â””â”€â”€ point_embedding.pt
        â””â”€â”€ lightning-drag-sd15-attn.bin
```

## ðŸš‹ Training

Start the WebUI using the following command. For detailed usage instructions, refer to [WebUI](./assets/webui-guide/webui.md)

### Launch the WebUI

```shell
python webui.py --colmap_dir <path to colmap dir> --gs_source <path to 3DGS ply> --output_dir <save path>
```

### Example

```shell
python webui.py --colmap_dir ./data/face/ --gs_source ./data/face/point_cloud.ply --output_dir result
```

### Training without WebUI (Recommended)

If you already have `gaussian_mask.pt` and `drag_points.json` saved from WebUI, you can train without WebUI by using the following command.

`--num_stages`
Specifies how many stages the entire drag editing process is divided into. Instead of applying a single large drag operation, the process is split into multiple smaller stages to achieve more stable and better results.

`--lora_only_first_stage`
If enabled, LoRA training will only occur during the first stage. Subsequent stages will reuse the pretrained LoRA from stage one instead of retraining it.

```shell
python drag_3d.py --config configs/main.yaml \
            --colmap_dir ./data/face/ \
            --gs_source ./data/face/point_cloud.ply \
            --point_dir ./data/face/export_1/drag_points.json \
            --mask_dir ./data/face/export_1/gaussian_mask.pt  \
            --output_dir result \
            --num_stages 3 \
            --lora_only_first_stage
```

<!-- ## Citation

```
@article{qu2025drag,
  title={Drag Your Gaussian: Effective Drag-Based Editing with Score Distillation for 3D Gaussian Splatting},
  author={Qu, Yansong and Chen, Dian and Li, Xinyang and Li, Xiaofan and Zhang, Shengchuan and Cao, Liujuan and Ji, Rongrong},
  journal={arXiv preprint arXiv:2501.18672},
  year={2025}
}
``` -->

<!-- ## License

Licensed under the CC BY-NC-SA 4.0 (Attribution-NonCommercial-ShareAlike 4.0 International)

The code is released for academic research use only.

If you have any questions, please contact me via [quyans@stu.xmu.edu.cn](mailto:quyans@stu.xmu.edu.cn). -->

## Acknowledgement

The implementation is base on [Drag Your Gaussian](https://github.com/Quyans/Drag-Your-Gaussian)
